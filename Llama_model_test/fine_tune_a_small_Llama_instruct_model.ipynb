{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3322b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 28 21:29:14 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b438f7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.41 in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.5.0)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.12/dist-packages (0.27.1)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.8.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.41) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.41) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.41) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers>=4.41) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers>=4.41) (8.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.41\" \"accelerate\" \"datasets\" \"trl\" \"peft\" \"bitsandbytes\" \"scikit-learn\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c5f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Access granted!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# TEMPORARY - DELETE THIS SCRIPT AFTER USE\n",
    "login(token=\"\")\n",
    "\n",
    "# Test access\n",
    "from huggingface_hub import model_info\n",
    "\n",
    "try:\n",
    "    info = model_info(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "    print(\"✓ Access granted!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Access denied: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b8d486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prompt': 'You are a classifier.\\nChoose exactly one label from: sci.space, rec.sport.baseball, talk.politics.misc, comp.graphics.\\n\\nText:\\n=== I aint talking the large or even the \"mining companies\" I am talking the small miners, the people who have themselves and a few employees (if at all).The people who go out every year and set up thier sluice box, and such and do mining the semi-old fashion way.. (okay they use modern methods toa point).  I am talking the guy who coem to Nome evry year, sets up his tent on the beach (the beach was washed away last year) and sets up his/her sluice box and goes at it \"mining\". I know the large corps, such as Alaska Gold Company, might complain to..  My opinions are what I learn at the local BS table..  My original thing/idea was that the way to get space mining was to allow the eco-freaks thier way.. As they have done with other mineral development. You can\\'t in many places can\\'t go to the bathroom in the woods without some form of regulation covering it..\\n\\nAnswer with only the label.',\n",
       "  'response': 'sci.space'},\n",
       " 800,\n",
       " 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from datasets import Dataset\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# Pick a small subset of categories to keep it simple\n",
    "categories = [\n",
    "    \"sci.space\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"comp.graphics\",\n",
    "]\n",
    "\n",
    "train = fetch_20newsgroups(\n",
    "    subset=\"train\", categories=categories, remove=(\"headers\", \"footers\", \"quotes\")\n",
    ")\n",
    "test = fetch_20newsgroups(\n",
    "    subset=\"test\", categories=categories, remove=(\"headers\", \"footers\", \"quotes\")\n",
    ")\n",
    "\n",
    "label_names = [\n",
    "    train.target_names[i] for i in range(len(train.target_names))\n",
    "]  # same order\n",
    "\n",
    "\n",
    "def make_rows(data, max_examples=800):\n",
    "    idxs = list(range(len(data.data)))\n",
    "    random.shuffle(idxs)\n",
    "    idxs = idxs[:max_examples]\n",
    "    rows = []\n",
    "    for i in idxs:\n",
    "        text = data.data[i].strip().replace(\"\\n\", \" \")\n",
    "        # keep examples short-ish to fit context\n",
    "        text = text[:1200]\n",
    "\n",
    "        label = data.target_names[data.target[i]]\n",
    "        prompt = (\n",
    "            \"You are a classifier.\\n\"\n",
    "            f\"Choose exactly one label from: {', '.join(categories)}.\\n\\n\"\n",
    "            f\"Text:\\n{text}\\n\\n\"\n",
    "            \"Answer with only the label.\"\n",
    "        )\n",
    "        rows.append({\"prompt\": prompt, \"response\": label})\n",
    "    return rows\n",
    "\n",
    "\n",
    "train_rows = make_rows(train, max_examples=800)  # small training set\n",
    "test_rows = make_rows(test, max_examples=200)  # small evaluation set\n",
    "\n",
    "train_ds = Dataset.from_list(train_rows)\n",
    "test_ds = Dataset.from_list(test_rows)\n",
    "\n",
    "train_ds[0], len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77eb9e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fb9ccb6b0b414381ca570a082039cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e173e2f29143c7a2dcef6f24dd4ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef151d6775f454c9da7b1598d2489ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e106a4f561c048a4b9c65e544cc59909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50d858989594217b9501bacfd7cd5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb846ebfdd94b0ca1888d384d0a5b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f39048bd5a4df0913820ce114dc8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=(\n",
    "        torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "    ),\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e11d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac198516c780468eb187db1c195268eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593ed95f0d20439cb2bb0618b1f0d54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 28 Jan 2026\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nYou are a classifier.\\nChoose exactly one label from: sci.space, rec.sport.baseball, talk.politics.misc, comp.graphics.\\n\\nText'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_chat_text(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "        {\"role\": \"assistant\", \"content\": example[\"response\"]},\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    return example\n",
    "\n",
    "\n",
    "train_ds2 = train_ds.map(to_chat_text)\n",
    "test_ds2 = test_ds.map(to_chat_text)\n",
    "train_ds2[0][\"text\"][:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d9e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0297d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:285: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085dfcfe68ad4c928eca46fe9a71b90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16344483a9e7469e8306bd19d70bb749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d82a32b6e142c69dad292def5d68c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 12:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.948571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.365052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.195619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.299173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.291632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "train_ds_only_text = train_ds2.remove_columns(\n",
    "    [c for c in train_ds2.column_names if c != \"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"llama_20news_lora\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,  # effective batch size = 16\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_length=512,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),  # use fp16 if bf16 not supported\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=[],\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds_only_text,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"llama_20news_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "377b93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "100%|██████████| 50/50 [00:49<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy@50: 0.00 (0/50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def predict_label(prompt, max_new_tokens=10):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(chat, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # greedy\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    # get last line-ish\n",
    "    pred = text.strip().split(\"\\n\")[-1].strip()\n",
    "    return pred\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    x = x.strip()\n",
    "    # sometimes the model may output extra punctuation\n",
    "    x = re.sub(r\"[^a-zA-Z\\.\\_]\", \"\", x)\n",
    "    return x\n",
    "\n",
    "\n",
    "correct = 0\n",
    "N = 50  # keep it small for speed\n",
    "for i in tqdm(range(N)):\n",
    "    prompt = test_rows[i][\"prompt\"]\n",
    "    gold = test_rows[i][\"response\"]\n",
    "    pred = predict_label(prompt)\n",
    "    if normalize(pred) == normalize(gold):\n",
    "        correct += 1\n",
    "\n",
    "print(f\"Accuracy@{N}: {correct/N:.2f} ({correct}/{N})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "304e337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciimportimportimportimportimportimportimportimportimport\n",
      "recimportimportimportimportimportimportimportimportimport\n",
      "talk        returnimportimportimportimportimportimportimport\n",
      "compimportimportimportimportimportimportimportimportimport\n"
     ]
    }
   ],
   "source": [
    "def ask(text):\n",
    "    prompt = (\n",
    "        \"Choose exactly one label from: sci.space, rec.sport.baseball, talk.politics.misc, comp.graphics.\\n\\n\"\n",
    "        f\"Text:\\n{text}\\n\\n\"\n",
    "        \"Answer with only the label.\"\n",
    "    )\n",
    "    return predict_label(prompt)\n",
    "\n",
    "\n",
    "print(ask(\"NASA announced a new launch window for a satellite mission next month.\"))\n",
    "print(ask(\"The pitcher threw a no-hitter and the fans went wild in the stadium.\"))\n",
    "print(ask(\"The new policy proposal is causing debate across different parties.\"))\n",
    "print(ask(\"Rendering performance improved after switching to a new shading technique.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07439ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"sci.space\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"comp.graphics\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122bc60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sci.space', 'rec.sport.baseball', 'talk.politics.misc', 'comp.graphics']\n"
     ]
    }
   ],
   "source": [
    "print(categories)\n",
    "assert categories == [\n",
    "    \"sci.space\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"talk.politics.misc\",\n",
    "    \"comp.graphics\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca59d4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a classifier.\n",
      "Choose exactly one label from: sci.space, rec.sport.baseball, talk.politics.misc, comp.graphics.\n",
      "\n",
      "Text:\n",
      "=== I aint talking the large or even the \"mining companies\" I am talking the small miners, the people who have themselves and a few employees (if at all).The people who go out every year and set up thier sluice box, and such and do mining the semi-old fashion way.. (okay they use modern methods toa point).  I am talking the guy who coem to Nome evry year, sets up his tent\n",
      "----\n",
      "sci.space\n"
     ]
    }
   ],
   "source": [
    "print(train_rows[0][\"prompt\"][:500])\n",
    "print(\"----\")\n",
    "print(train_rows[0][\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bdcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
